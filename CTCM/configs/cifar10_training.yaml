# Configuration for training consistency model from scratch
# Complete standalone configuration without defaults

model:
  type: "NO-CTCM"

  backbone:
    type: "unet"
    use_simple: false
    in_channels: 3
    out_channels: 3
    channels: 128
    channel_mult: [1, 2, 2, 2]
    num_res_blocks: 2
    attention_resolutions: [16, 8]
    dropout: 0.0
    use_checkpoint: false
    num_heads: 4
    num_head_channels: 64
    use_scale_shift_norm: true
    resblock_updown: true

  neural_operator:
    use_fno: true
    fno_modes: 16
    fno_width: 128
    fno_layers: 4
    spectral_normalization: true

  time_conditioning:
    type: "trigflow"
    embed_dim: 256
    positional_encoding: true
    fourier_scale: 1.0
    adaptive_norm: true
    pixel_norm: true

  consistency:
    parameterization: "v-prediction"
    sigma_data: 0.5
    sigma_min: 0.002
    sigma_max: 80.0
    rho: 7.0

data:
  dataset: "cifar10"
  image_size: 32
  channels: 3
  flip_prob: 0.5
  normalize: true
  mean: [0.5, 0.5, 0.5]
  std: [0.5, 0.5, 0.5]
  num_workers: 4

training:
  mode: "consistency_training"
  batch_size: 128
  total_iterations: 800000

  # CTM-style hybrid training
  hybrid_training:
    enabled: true
    consistency_weight: 1.0
    dsm_weight: 0.1
    dsm_start_iter: 100000
    adversarial_weight: 0.0
    adversarial_start_iter: 400000

  # Multi-scale training
  multiscale:
    enabled: true
    scales: [1, 2, 4]
    scale_weights: [1.0, 0.5, 0.25]

  # Advanced progressive schedule
  progressive:
    enabled: true
    initial_discretization: 4
    final_discretization: 1280
    schedule_type: "geometric"
    doubling_iterations: 50000

  # Curriculum learning
  curriculum:
    enabled: true
    noise_schedule: "cosine"
    start_sigma: 80.0
    end_sigma: 0.002
    warmup_iterations: 10000

  # Optimizer settings
  optimizer:
    type: "adamw"
    lr: 2e-4
    betas: [0.9, 0.99]
    weight_decay: 1e-2
    eps: 1e-8

  lr_scheduler:
    type: "cosine"
    warmup_steps: 5000
    min_lr: 1e-6

  loss:
    type: "pseudo_huber"
    c: 0.01
    reduction: "mean"

  # Variance reduction
  variance_reduction:
    enabled: true
    method: "control_variates"
    baseline_momentum: 0.99

  # Regularization
  regularization:
    spectral_norm: true
    weight_standardization: true
    dropout_rate: 0.1

  ema:
    enabled: true
    decay: 0.9999
    update_every: 10

  gradient_clip: 1.0

  checkpoint:
    save_every: 5000
    keep_last: 5

  log_every: 100
  sample_every: 1000

  distributed:
    enabled: false
    backend: "nccl"

sampling:
  num_steps: 1
  solver: "euler"
  guidance_scale: 0.0

evaluation:
  batch_size: 256
  num_samples: 50000
  metrics: ["fid", "is", "precision", "recall"]

wandb:
  enabled: false
  project: "neural-consistency-model"
  entity: null
  tags: ["cifar10", "from_scratch", "ctm_style"]

paths:
  data_dir: "./data"
  checkpoint_dir: "./checkpoints"
  sample_dir: "./samples"
  log_dir: "./logs"